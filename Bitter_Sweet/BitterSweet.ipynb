{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('data/processed_mean_bitter_train.csv')\n",
    "# # Map the Taste column\n",
    "# df[\"Taste\"] = df[\"Taste\"].map({\"Sweet\": 2, \"Bitter\": 1}).fillna(0).astype(int)\n",
    "\n",
    "# # Save the modified CSV\n",
    "# df.to_csv(\"data/BitterSweet.csv\", index=False)\n",
    "\n",
    "# print(\"Taste column updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from boruta import BorutaPy\n",
    "from joblib import dump, load\n",
    "bitter_features = pd.read_csv('data/BitterSweet.csv')\n",
    "bitter_features = bitter_features.iloc[:2235]\n",
    "replaced_data_mean = bitter_features.iloc[:, 7:]\n",
    "\n",
    "y = bitter_features['Taste']\n",
    "\n",
    "# Features\n",
    "X = replaced_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('bitter/bitter_boruta.p','rb')\n",
    "\n",
    "boruta = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = X_test.reindex(columns=boruta, fill_value=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitter : \n",
      "[False  True False False  True False  True  True  True False  True  True\n",
      " False  True False  True  True False False False False False False False\n",
      " False  True False False False False False  True  True False False False\n",
      " False  True False  True  True False False  True False False  True False\n",
      " False False  True  True False False  True False  True False  True False\n",
      " False False False  True  True False  True False False  True False False\n",
      " False False False False  True False False False  True False False False\n",
      "  True False  True  True False False False False False False False False\n",
      " False False False False False  True  True  True False False False False\n",
      "  True False False False  True False False  True False  True False  True\n",
      " False False False False False False False False  True  True False False\n",
      "  True  True False False  True False False False False  True False False\n",
      "  True False False  True False False False False  True False False  True\n",
      " False False False False  True False False False False False  True False\n",
      " False  True  True False False  True False  True  True  True  True  True\n",
      " False False False  True False  True False False  True  True  True False\n",
      " False  True  True False  True  True False False  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False False False False False  True False  True  True  True\n",
      " False False  True  True False False False False  True False False False\n",
      " False False False False False False  True False  True  True False False\n",
      " False  True False  True False  True False  True False  True False False\n",
      " False  True  True False False False False False False False False False\n",
      " False  True  True  True False False False False  True False False  True\n",
      " False False  True  True  True False  True  True False  True False False\n",
      "  True  True False False  True False  True False False False False False\n",
      " False  True  True  True False False False  True False False False False\n",
      "  True  True False False False False  True False False False False  True\n",
      " False False  True False  True False False  True  True  True  True False\n",
      "  True  True  True False False False False False False  True  True  True\n",
      " False False False False  True  True False False  True False  True False\n",
      "  True  True  True False  True False  True False  True False False  True\n",
      " False False  True  True  True False False False False  True False False\n",
      " False  True False False False  True False False  True False False False\n",
      " False False False False  True False False  True False False  True  True\n",
      " False  True  True False False False False  True False False False False\n",
      " False False  True]\n",
      "[0.00982226 0.93661392 0.05744923 0.16298858 0.87184776 0.01379139\n",
      " 0.99305584 0.98866667 0.89145828 0.00955856 0.56747934 0.67876995\n",
      " 0.0592297  0.94773623 0.04366384 0.84282926 0.96973878 0.01693026\n",
      " 0.01946822 0.16499964 0.03778297 0.07167987 0.03034849 0.11496427\n",
      " 0.29811154 0.97466758 0.02318681 0.01491541 0.01344566 0.02928252\n",
      " 0.09226681 0.9912     0.67456639 0.16836393 0.04274219 0.02044745\n",
      " 0.01563624 0.81359707 0.03285966 0.98007607 0.90776326 0.02203245\n",
      " 0.03352316 0.90149368 0.22491322 0.05436437 0.94452403 0.03665271\n",
      " 0.01887063 0.0114767  0.97893897 0.98713333 0.07843309 0.04888128\n",
      " 0.79633028 0.03494696 0.9548227  0.06774525 0.91536174 0.01494437\n",
      " 0.03195505 0.07866613 0.03928743 0.99025455 0.90077652 0.01045568\n",
      " 0.61453907 0.02218567 0.03828483 0.65768113 0.03632279 0.05621408\n",
      " 0.06148406 0.02610221 0.02309879 0.02798106 1.         0.01508261\n",
      " 0.03679608 0.00941481 0.89698119 0.01414124 0.06349387 0.02988232\n",
      " 0.68814098 0.26721791 0.95819519 0.96598001 0.18894511 0.04456173\n",
      " 0.07624693 0.06148689 0.06879192 0.02586103 0.30256454 0.01656334\n",
      " 0.06033328 0.02059244 0.03063649 0.08324772 0.02422918 0.96831941\n",
      " 0.9036396  0.65916217 0.03443402 0.04284176 0.02338963 0.12594514\n",
      " 0.55796429 0.04390087 0.01733992 0.02006249 0.88852187 0.01289621\n",
      " 0.05385938 0.99832619 0.02568642 0.99618086 0.25180656 0.92175991\n",
      " 0.02034237 0.03936881 0.02832149 0.05951843 0.09812247 0.03484018\n",
      " 0.04428215 0.05156382 0.72679503 0.8614958  0.29412404 0.0553818\n",
      " 0.98194469 0.96278    0.02915265 0.4004739  0.82098035 0.03857383\n",
      " 0.48531493 0.01043995 0.01197279 0.9796     0.02104831 0.05109513\n",
      " 0.81758381 0.06837789 0.02082563 0.90318821 0.3103832  0.01719404\n",
      " 0.0132764  0.01005744 0.94995527 0.01423548 0.01001096 0.92579133\n",
      " 0.01275206 0.00987571 0.02147386 0.01781452 0.85129287 0.01839543\n",
      " 0.01729437 0.48971658 0.01105751 0.02054485 0.67261806 0.00490542\n",
      " 0.01826431 0.93668935 0.99728682 0.02882055 0.02174721 0.99446667\n",
      " 0.02555123 0.69867754 0.92611529 0.99839286 0.55597815 0.9657861\n",
      " 0.09526721 0.04468507 0.01408316 0.83515666 0.03857515 0.98297906\n",
      " 0.02109203 0.01767919 0.98334858 0.77607683 0.94264523 0.01099902\n",
      " 0.04309231 0.94849894 0.93518066 0.00503934 0.63998485 0.98652838\n",
      " 0.025042   0.01103651 0.95513926 0.99633333 0.01237967 0.0301573\n",
      " 0.03252248 0.02317126 0.01289321 0.01946731 0.14773462 0.07137719\n",
      " 0.00948306 0.03048417 0.01827575 0.01860049 0.0188475  0.02998157\n",
      " 0.06414984 0.09339213 0.02454892 0.18816997 0.03423194 0.02978245\n",
      " 0.03463594 0.96468    0.01045568 0.02166675 0.04084776 0.01367115\n",
      " 0.97386667 0.18103247 0.05845697 0.01195019 0.01106796 0.35843206\n",
      " 0.18722766 0.55124438 0.01058799 0.9356163  0.93526204 0.99793333\n",
      " 0.01181449 0.07891995 0.99249394 0.97909758 0.0253745  0.03444315\n",
      " 0.0290479  0.02357081 0.95985683 0.0356759  0.03139397 0.47503412\n",
      " 0.01900236 0.03268913 0.16397804 0.07506546 0.34462497 0.10370651\n",
      " 0.80385974 0.2128619  0.99773333 0.72679503 0.04033383 0.00941481\n",
      " 0.02725269 0.97355514 0.00805821 0.98102904 0.02025296 0.9864\n",
      " 0.00941481 0.97745075 0.00941481 0.83396619 0.11158631 0.12236684\n",
      " 0.0099472  0.98286667 0.78225595 0.03934103 0.00897287 0.02778866\n",
      " 0.01519998 0.10820449 0.01551318 0.0206115  0.02415487 0.03873411\n",
      " 0.02377566 0.85285459 0.96016471 0.89929792 0.02091623 0.02972298\n",
      " 0.04315617 0.04171218 0.60025742 0.0258806  0.14754786 0.99059231\n",
      " 0.05584158 0.01640943 0.97860434 0.79008667 0.99064947 0.03370975\n",
      " 0.77193873 0.97504053 0.00977472 0.77765418 0.06586963 0.23537161\n",
      " 0.9549018  0.99826667 0.01787335 0.01010535 0.95546832 0.01123327\n",
      " 0.59683934 0.01802821 0.10763568 0.02809312 0.0618763  0.05011711\n",
      " 0.34979334 0.99244242 0.967471   0.91911594 0.03351291 0.1771583\n",
      " 0.05620405 0.93690099 0.16486478 0.0139824  0.02714071 0.01560876\n",
      " 0.98915381 0.95392056 0.0120404  0.11222803 0.01200438 0.00973414\n",
      " 0.85229102 0.0110268  0.01156347 0.02030588 0.01098282 0.78600337\n",
      " 0.43660826 0.01007122 0.87907307 0.28428432 0.54272829 0.01654027\n",
      " 0.00964579 0.98742126 0.8694326  0.7328321  0.87828681 0.3337164\n",
      " 0.84322626 0.93813191 0.94122266 0.07098072 0.48670927 0.1274992\n",
      " 0.05289795 0.02796477 0.03337186 0.54875604 0.98967286 0.96661068\n",
      " 0.02635312 0.00942215 0.02105493 0.02206916 0.99573333 0.76107159\n",
      " 0.05812884 0.06739055 0.67487986 0.02424796 0.98761019 0.01459787\n",
      " 0.9885521  0.99613333 0.99112619 0.02634324 0.97644231 0.00941481\n",
      " 0.96204996 0.02941751 0.93724205 0.01357837 0.02778158 0.93703576\n",
      " 0.02133363 0.35512799 0.97930417 0.88818398 0.85233777 0.02481174\n",
      " 0.0737644  0.01752053 0.48670927 0.90511996 0.06603445 0.01466636\n",
      " 0.02395116 0.98582667 0.02386556 0.0675347  0.01007918 0.97644529\n",
      " 0.39564133 0.00617018 0.97412636 0.02760111 0.01146345 0.03327246\n",
      " 0.10962654 0.02161022 0.01532099 0.12876245 0.86180322 0.00975668\n",
      " 0.02703639 0.93942251 0.03249369 0.01134002 0.96672616 0.53421172\n",
      " 0.0450345  0.9609294  0.50394716 0.02098826 0.02863624 0.0157581\n",
      " 0.01304051 0.73314203 0.38791978 0.03535477 0.06066573 0.01794797\n",
      " 0.0209165  0.04529471 0.85829196]\n"
     ]
    }
   ],
   "source": [
    "file = open('bitter/mean.p','rb')\n",
    "import pickle\n",
    "best_model =pickle.load(file)\n",
    "\n",
    "y_pred_bitter = best_model.predict(X_transformed)\n",
    "y_pred_proba_bitter = best_model.predict_proba(X_transformed)[:, 1]\n",
    "print(\"Bitter : \")\n",
    "print(y_pred_bitter)\n",
    "print(y_pred_proba_bitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('sweet/sweet_boruta.p','rb')\n",
    "\n",
    "boruta_sweet = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_sweet = X_test.reindex(columns=boruta_sweet, fill_value=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweet : \n",
      "[ True False False  True False  True False False False  True False False\n",
      "  True False  True False False  True  True False  True  True  True  True\n",
      " False False  True  True  True  True  True False False False  True  True\n",
      "  True False False False False  True  True False  True False False  True\n",
      "  True  True False False  True  True False False False  True False  True\n",
      " False False  True False False  True False  True False False  True  True\n",
      " False  True  True  True False  True  True  True False  True  True  True\n",
      " False False False False False  True  True  True False  True  True False\n",
      "  True  True  True False  True False False False  True  True  True False\n",
      " False  True  True  True False  True  True False  True False  True False\n",
      "  True  True  True False  True  True False  True False False  True False\n",
      " False False  True  True False  True False  True  True False  True  True\n",
      " False  True  True False False  True  True  True False  True  True False\n",
      "  True  True  True  True False  True  True False  True  True False  True\n",
      "  True False False False  True False  True False False False False False\n",
      " False  True  True False  True False  True  True False False False  True\n",
      "  True False False  True False False  True  True False False  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True False False False\n",
      " False  True False False  True  True  True  True False  True  True False\n",
      "  True False False  True  True  True False  True False False  True  True\n",
      "  True False  True False  True False  True False  True False False False\n",
      "  True False False  True  True  True  True False  True  True  True False\n",
      "  True False False False  True  True  True  True False  True False False\n",
      " False  True False False False False False False  True False  True False\n",
      " False False  True  True False  True False  True False  True  True False\n",
      " False False False False  True False  True False  True  True False  True\n",
      " False False  True False  True  True  True  True  True  True  True False\n",
      " False  True False  True False  True  True False False False False False\n",
      " False False False  True False  True  True  True  True False False False\n",
      "  True  True  True  True False False  True False False  True False  True\n",
      " False False False  True False  True False  True False  True  True False\n",
      "  True  True False False False  True  True  True False False False  True\n",
      "  True False False  True  True False  True  True False  True  True False\n",
      " False False  True  True False  True  True False  True  True False False\n",
      "  True False False  True  True  True  True False  True  True False  True\n",
      "  True  True False]\n",
      "[0.97625    0.07641667 0.08583333 0.77766667 0.0515     0.89862121\n",
      " 0.03308333 0.012      0.07783333 0.97591667 0.04170455 0.05257576\n",
      " 0.90437121 0.02907576 0.91808333 0.012      0.03008333 0.97725\n",
      " 0.95083333 0.07440909 0.92874242 0.90828788 0.79991667 0.74603788\n",
      " 0.10795455 0.06358333 0.95       0.97616667 0.95028788 0.80116667\n",
      " 0.9455     0.01875    0.19733333 0.42925    0.77737121 0.96078788\n",
      " 0.95658333 0.04312121 0.28141667 0.07511364 0.03020455 0.91107576\n",
      " 0.81995455 0.10791667 0.93816667 0.07858333 0.04736364 0.88366667\n",
      " 0.95375    0.97920455 0.03241667 0.05120455 0.97266667 0.97316667\n",
      " 0.17670455 0.03245455 0.03295455 0.90462121 0.03187121 0.95315909\n",
      " 0.08408333 0.04699242 0.97291667 0.01791667 0.09833333 0.93175\n",
      " 0.08828788 0.88162121 0.04408333 0.29672727 0.91175    0.88608333\n",
      " 0.05208333 0.80575    0.94675    0.81349242 0.00520455 0.9235\n",
      " 0.93308333 0.97891667 0.07766667 0.92278788 0.89891667 0.96\n",
      " 0.20727273 0.19653788 0.21608333 0.03728788 0.01565909 0.91033333\n",
      " 0.887      0.84225    0.31793939 0.92925    0.69620455 0.03425\n",
      " 0.77266667 0.95075    0.85675    0.03766667 0.87391667 0.03616667\n",
      " 0.03790909 0.06849242 0.53153788 0.9025     0.95925    0.23337121\n",
      " 0.13337121 0.75008333 0.97162121 0.58891667 0.18458333 0.97957576\n",
      " 0.90458333 0.02416667 0.93958333 0.01740909 0.68928788 0.06987121\n",
      " 0.96308333 0.96016667 0.6415     0.28286364 0.82258333 0.90383333\n",
      " 0.0575303  0.92983333 0.03237121 0.10325    0.87575    0.23315909\n",
      " 0.00833333 0.02957576 0.92883333 0.92783333 0.02112121 0.92275\n",
      " 0.41525    0.97366667 0.97308333 0.02812121 0.87925    0.92508333\n",
      " 0.05508333 0.88883333 0.53541667 0.09975    0.06069697 0.79833333\n",
      " 0.93716667 0.97691667 0.06603788 0.98116667 0.97475    0.07633333\n",
      " 0.95791667 0.97566667 0.91920455 0.96908333 0.01762121 0.96983333\n",
      " 0.96733333 0.08       0.97141667 0.95337121 0.32445455 0.81820455\n",
      " 0.93275    0.121      0.04783333 0.08358333 0.82991667 0.00716667\n",
      " 0.86525    0.32991667 0.05266667 0.02258333 0.02912121 0.03132576\n",
      " 0.10815909 0.7305     0.96825    0.013      0.81420455 0.10665909\n",
      " 0.95703788 0.76612121 0.01233333 0.18833333 0.02112121 0.97275\n",
      " 0.81066667 0.27637121 0.02420455 0.91358333 0.03287121 0.00812121\n",
      " 0.96558333 0.974      0.07762121 0.01778788 0.95941667 0.95778788\n",
      " 0.23666667 0.94591667 0.81853788 0.93441667 0.90249242 0.94108333\n",
      " 0.97991667 0.95941667 0.94433333 0.97478788 0.72815909 0.95858333\n",
      " 0.75962121 0.94508333 0.93728788 0.9485     0.92995455 0.92703788\n",
      " 0.95058333 0.02462121 0.93175    0.9485     0.90491667 0.96725\n",
      " 0.02737121 0.9265     0.57803788 0.96586364 0.97816667 0.90841667\n",
      " 0.94766667 0.05016667 0.97866667 0.08516667 0.01475    0.01240909\n",
      " 0.03837121 0.86078788 0.03783333 0.02120455 0.90091667 0.953\n",
      " 0.76203788 0.89916667 0.08125    0.78608333 0.87045455 0.04253788\n",
      " 0.94891667 0.39358333 0.08444697 0.95791667 0.90508333 0.78583333\n",
      " 0.20883333 0.94533333 0.03444697 0.03237121 0.84712121 0.97891667\n",
      " 0.93966667 0.035      0.96862121 0.0265     0.96833333 0.01378788\n",
      " 0.97891667 0.05258333 0.97891667 0.16481818 0.03065909 0.03612121\n",
      " 0.973      0.01183333 0.01633333 0.9315     0.93353788 0.73012121\n",
      " 0.95758333 0.07457576 0.96720455 0.97791667 0.8565     0.08125\n",
      " 0.85583333 0.07945455 0.0225     0.077      0.96833333 0.93020455\n",
      " 0.68175    0.56578788 0.01741667 0.88020455 0.03662121 0.00508333\n",
      " 0.07715909 0.97441667 0.022      0.09525    0.05375    0.08191667\n",
      " 0.02283333 0.024      0.93275    0.04166667 0.84815909 0.07108333\n",
      " 0.05770455 0.02458333 0.96962121 0.97291667 0.07666667 0.97275\n",
      " 0.23775    0.97391667 0.03328788 0.96716667 0.8925     0.13208333\n",
      " 0.28187121 0.01483333 0.02774242 0.09587121 0.93083333 0.04091667\n",
      " 0.89       0.05612121 0.9495     0.93020455 0.04315909 0.94137121\n",
      " 0.1385     0.03833333 0.96766667 0.24053788 0.96433333 0.9765\n",
      " 0.81525    0.96091667 0.97425    0.9705     0.97641667 0.06165909\n",
      " 0.09858333 0.96233333 0.12008333 0.80025    0.06520455 0.94133333\n",
      " 0.97891667 0.01141667 0.08145455 0.16995455 0.02637121 0.11412121\n",
      " 0.13699242 0.05816667 0.11333333 0.90566667 0.03575    0.82995455\n",
      " 0.90658333 0.6575     0.92775    0.31790909 0.02128788 0.11490909\n",
      " 0.95208333 0.97537121 0.93366667 0.97633333 0.01715909 0.00566667\n",
      " 0.91575    0.07       0.0675     0.90095455 0.01228788 0.96266667\n",
      " 0.01608333 0.05132576 0.042      0.92225    0.04387121 0.97891667\n",
      " 0.08607576 0.89308333 0.24128788 0.97390909 0.94591667 0.08220455\n",
      " 0.94175    0.63291667 0.14916667 0.22011364 0.04366667 0.97366667\n",
      " 0.87491667 0.97708333 0.03575    0.13290909 0.06841667 0.96175\n",
      " 0.96183333 0.04295455 0.0635     0.59033333 0.97875    0.04316667\n",
      " 0.86508333 0.903      0.03362121 0.91891667 0.88828788 0.2045\n",
      " 0.14108333 0.11403788 0.93262121 0.91941667 0.189      0.97483333\n",
      " 0.96766667 0.05470455 0.88191667 0.973      0.03107576 0.03125\n",
      " 0.9025     0.12208333 0.08887121 0.92857576 0.94916667 0.95224242\n",
      " 0.97166667 0.13916667 0.5337803  0.9175     0.0477803  0.91583333\n",
      " 0.96466667 0.87625    0.0475    ]\n"
     ]
    }
   ],
   "source": [
    "file = open('sweet/mean.p','rb')\n",
    "import pickle\n",
    "best_model =pickle.load(file)\n",
    "\n",
    "y_pred_sweet = best_model.predict(X_transformed_sweet)\n",
    "y_pred_proba_sweet = best_model.predict_proba(X_transformed_sweet)[:, 1]\n",
    "print(\"Sweet : \")\n",
    "print(y_pred_sweet)\n",
    "print(y_pred_proba_sweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2000):\n",
    "#     print(y[i],\" \",y_pred_bitter[i],\" \",y_pred_sweet[i],' ',y_pred_proba_bitter[i],\" \",y_pred_proba_sweet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9463\n",
      "Precision: 0.8840\n",
      "Recall (Sensitivity): 0.9329\n",
      "F1-score: 0.9035\n",
      "ROC-AUC Score: 0.9934\n",
      "AUPR Score: 0.9671\n",
      "Non Error Rate: 0.9530\n",
      "Specificity: 0.9048\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78        42\n",
      "           1       0.97      0.93      0.95       158\n",
      "           2       0.99      0.96      0.98       247\n",
      "\n",
      "    accuracy                           0.95       447\n",
      "   macro avg       0.88      0.93      0.90       447\n",
      "weighted avg       0.95      0.95      0.95       447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report,\n",
    "    average_precision_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Convert predictions into a single class label (tasteless: 0, bitter: 1, sweet: 2)\n",
    "y_pred = []\n",
    "for bitter, sweet, bittprob, sweetprob in zip(y_pred_bitter, y_pred_sweet, y_pred_proba_bitter,y_pred_proba_sweet):\n",
    "    if bitter & sweet : \n",
    "        if bittprob > sweetprob:\n",
    "            y_pred.append(1)\n",
    "        else : \n",
    "            y_pred.append(2)\n",
    "    elif bitter:\n",
    "        y_pred.append(1)  # Bitter\n",
    "    elif sweet:\n",
    "        y_pred.append(2)  # Sweet\n",
    "    else:\n",
    "        y_pred.append(0)  # Tasteless\n",
    "\n",
    "# Convert probability predictions into a single class probability (ensuring they sum to 1)\n",
    "y_pred_proba = np.array([\n",
    "    [1 - (p_bitter + p_sweet), p_bitter, p_sweet] for p_bitter, p_sweet in zip(y_pred_proba_bitter, y_pred_proba_sweet)\n",
    "])\n",
    "\n",
    "# Ensure numerical stability: clip probabilities between [0,1] and normalize\n",
    "y_pred_proba = np.clip(y_pred_proba, 0, 1)\n",
    "y_pred_proba /= y_pred_proba.sum(axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')  # Fixed by ensuring probabilities sum to 1\n",
    "aupr = average_precision_score(y_test, y_pred_proba, average='macro')\n",
    "classification_rep = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "# Confusion matrix to compute specificity and sensitivity\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn = conf_matrix[0, 0]  # True Negatives for tasteless\n",
    "fp = conf_matrix[0, 1:].sum()  # False Positives for tasteless\n",
    "fn = conf_matrix[1:, 0].sum()  # False Negatives for tasteless\n",
    "tp = conf_matrix[1:, 1:].sum()  # True Positives for bitter and sweet\n",
    "\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate (Recall)\n",
    "non_error_rate = (tp + tn) / conf_matrix.sum()  # Non-error rate\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"AUPR Score: {aupr:.4f}\")\n",
    "print(f\"Non Error Rate: {non_error_rate:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
