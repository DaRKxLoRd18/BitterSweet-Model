{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from boruta import BorutaPy\n",
    "from joblib import dump, load\n",
    "bitter_features = pd.read_csv('output/mean/processed_mean_bitter_train.csv')\n",
    "\n",
    "replaced_data_mean = bitter_features.iloc[:, 9:]\n",
    "\n",
    "y = bitter_features['Bitter']\n",
    "\n",
    "# Features\n",
    "X = replaced_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Boruta model\n",
    "boruta = load('model/boruta/boruta_mean_model.joblib')\n",
    "\n",
    "# Transform the dataset using the loaded model\n",
    "X_transformed = boruta.transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_feature_names = X.columns[boruta.support_].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('features/bitter/bitter_boruta.p','wb')\n",
    "pickle.dump(significant_feature_names,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 2 is smaller than n_iter=20. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "\n",
      "AdaBoost:\n",
      "Best Hyperparameters: {'n_estimators': 300, 'learning_rate': 0.1, 'algorithm': 'SAMME'}\n",
      "Accuracy: 0.8451327433628318\n",
      "Precision: 0.9043478260869565\n",
      "Recall: 0.6380368098159509\n",
      "Sensitivity: 0.6380368098159509\n",
      "F1-score: 0.7482014388489209\n",
      "ROC-AUC Score: 0.9056297365571995\n",
      "AUPR Score: 0.8462968836665203\n",
      "Non Error Rate (NER): 0.9619377162629758\n",
      "Specificity (SP): 0.9619377162629758\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.96      0.89       289\n",
      "        True       0.90      0.64      0.75       163\n",
      "\n",
      "    accuracy                           0.85       452\n",
      "   macro avg       0.86      0.80      0.82       452\n",
      "weighted avg       0.85      0.85      0.84       452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix, classification_report\n",
    ")\n",
    "# Hyperparameter grid for AdaBoost\n",
    "adaboost_param_grid = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"learning_rate\": [0.01, 0.1],\n",
    "    \"algorithm\": [\"SAMME\"]\n",
    "}\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "ada_random = RandomizedSearchCV(\n",
    "    estimator=ada,\n",
    "    param_distributions=adaboost_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "ada_random.fit(X_train, y_train)\n",
    "best_ada = ada_random.best_estimator_\n",
    "\n",
    "y_pred_ada = best_ada.predict(X_test)\n",
    "y_pred_proba_ada = best_ada.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nAdaBoost:\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_ada).ravel()\n",
    "ner = fp / (tn + fp) if (tn + fp) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(\"Best Hyperparameters:\", ada_random.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_ada))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_ada))\n",
    "print(\"Sensitivity:\", recall_score(y_test, y_pred_ada))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_ada))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba_ada))\n",
    "print(\"AUPR Score:\", average_precision_score(y_test, y_pred_proba_ada))\n",
    "print(\"Non Error Rate (NER):\", 1 - ner)\n",
    "print(\"Specificity (SP):\", specificity)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_ada))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('final_model/adaboost.p','wb')\n",
    "pickle.dump(best_ada,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8451327433628318\n",
      "Precision (P): 0.9043478260869565\n",
      "Recall (R): 0.6380368098159509\n",
      "Sensitivity (Sn): 0.6380368098159509\n",
      "F1-score (F1): 0.7482014388489209\n",
      "ROC-AUC Score (AuROC): 0.9056297365571995\n",
      "AUPR Score (AuPR): 0.8462968836665203\n",
      "Non Error Rate (NER): 0.9619377162629758\n",
      "Specificity (SP): 0.9619377162629758\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.96      0.89       289\n",
      "        True       0.90      0.64      0.75       163\n",
      "\n",
      "    accuracy                           0.85       452\n",
      "   macro avg       0.86      0.80      0.82       452\n",
      "weighted avg       0.85      0.85      0.84       452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file = open('final_model/adaboost.p','rb')\n",
    "\n",
    "best_model =pickle.load(file)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "aupr = average_precision_score(y_test, y_pred_proba)  # AUPR\n",
    "sensitivity = recall  # Sensitivity (Sn) is the same as recall\n",
    "\n",
    "# Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Calculate NER and Specificity\n",
    "ner = fp / (tn + fp) if (tn + fp) > 0 else 0  # Avoid division by zero\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (P):\", precision)\n",
    "print(\"Recall (R):\", recall)\n",
    "print(\"Sensitivity (Sn):\", sensitivity)\n",
    "print(\"F1-score (F1):\", f1)\n",
    "print(\"ROC-AUC Score (AuROC):\", roc_auc)\n",
    "print(\"AUPR Score (AuPR):\", aupr)\n",
    "print(\"Non Error Rate (NER):\", 1-ner)\n",
    "print(\"Specificity (SP):\", specificity)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
