{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from boruta import BorutaPy\n",
    "from joblib import dump, load\n",
    "bitter_features = pd.read_csv('test.csv')\n",
    "\n",
    "replaced_data_mean = bitter_features.iloc[:, 10:]\n",
    "\n",
    "y = bitter_features['Taste']\n",
    "\n",
    "# Features\n",
    "X = replaced_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('features/boruta.p','rb')\n",
    "\n",
    "boruta = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = X.reindex(columns=boruta).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "y_onehot = tf.keras.utils.to_categorical(y, num_classes=len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score, classification_report, roc_auc_score,\n",
    "#     precision_score, recall_score, f1_score, average_precision_score\n",
    "# )\n",
    "# import numpy as np\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_onehot, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n",
      "AUPR Score: 0.8808\n",
      "ROC-AUC Score: 0.9195\n",
      "F1-Score: 0.8388\n",
      "Non-Error Rate: 0.8965\n",
      "Recall (Sensitivity): 0.8447\n",
      "Specificity: 0.8774\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "loaded_model = load_model(\"deep_learning_model.keras\")\n",
    "# Make Predictions\n",
    "y_pred_prob = loaded_model.predict(X_transformed)  # Probabilities\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
    "y_true = np.argmax(y_onehot, axis=1)  # True class labels\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(y_onehot, y_pred_prob, multi_class=\"ovr\")  # One-vs-Rest ROC-AUC\n",
    "aupr = average_precision_score(y_onehot, y_pred_prob, average=\"weighted\")  # AUPR Score\n",
    "classification_rep = classification_report(y_true, y_pred)\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# True Positives (TP) per class\n",
    "tp = np.diag(cm)\n",
    "\n",
    "# False Negatives (FN) per class\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "\n",
    "# False Positives (FP) per class\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "\n",
    "# True Negatives (TN) per class\n",
    "tn = cm.sum() - (tp + fn + fp)  # Corrected TN computation\n",
    "\n",
    "sensitivity = (tp / (tp + fn)).mean()  # Sensitivity = Recall\n",
    "specificity = (tn / (tn + fp)).mean()  # Avoid division by zero\n",
    "non_error_rate = (tp + tn) / cm.sum()  # Non Error Rate\n",
    "\n",
    "print(f\"AUPR Score: {aupr:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Non-Error Rate: {non_error_rate.mean():.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n",
      "AUPR Score: 0.9014\n",
      "ROC-AUC Score: 0.9326\n",
      "F1-Score: 0.8616\n",
      "Non-Error Rate: 0.9089\n",
      "Recall (Sensitivity): 0.8634\n",
      "Specificity: 0.9034\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "loaded_model = load_model(\"deep_learning_model2.keras\")\n",
    "# Make Predictions\n",
    "y_pred_prob = loaded_model.predict(X_transformed)  # Probabilities\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
    "y_true = np.argmax(y_onehot, axis=1)  # True class labels\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(y_onehot, y_pred_prob, multi_class=\"ovr\")  # One-vs-Rest ROC-AUC\n",
    "aupr = average_precision_score(y_onehot, y_pred_prob, average=\"weighted\")  # AUPR Score\n",
    "classification_rep = classification_report(y_true, y_pred)\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# True Positives (TP) per class\n",
    "tp = np.diag(cm)\n",
    "\n",
    "# False Negatives (FN) per class\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "\n",
    "# False Positives (FP) per class\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "\n",
    "# True Negatives (TN) per class\n",
    "tn = cm.sum() - (tp + fn + fp)  # Corrected TN computation\n",
    "\n",
    "sensitivity = (tp / (tp + fn)).mean()  # Sensitivity = Recall\n",
    "specificity = (tn / (tn + fp)).mean()  # Avoid division by zero\n",
    "non_error_rate = (tp + tn) / cm.sum()  # Non Error Rate\n",
    "\n",
    "print(f\"AUPR Score: {aupr:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Non-Error Rate: {non_error_rate.mean():.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step\n",
      "AUPR Score: 0.9162\n",
      "ROC-AUC Score: 0.9357\n",
      "F1-Score: 0.8515\n",
      "Non-Error Rate: 0.9048\n",
      "Recall (Sensitivity): 0.8571\n",
      "Specificity: 0.8821\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "loaded_model = load_model(\"deep_learning_model_cnn.keras\")\n",
    "# Make Predictions\n",
    "y_pred_prob = loaded_model.predict(X_transformed)  # Probabilities\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
    "y_true = np.argmax(y_onehot, axis=1)  # True class labels\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(y_onehot, y_pred_prob, multi_class=\"ovr\")  # One-vs-Rest ROC-AUC\n",
    "aupr = average_precision_score(y_onehot, y_pred_prob, average=\"weighted\")  # AUPR Score\n",
    "classification_rep = classification_report(y_true, y_pred)\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# True Positives (TP) per class\n",
    "tp = np.diag(cm)\n",
    "\n",
    "# False Negatives (FN) per class\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "\n",
    "# False Positives (FP) per class\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "\n",
    "# True Negatives (TN) per class\n",
    "tn = cm.sum() - (tp + fn + fp)  # Corrected TN computation\n",
    "\n",
    "sensitivity = (tp / (tp + fn)).mean()  # Sensitivity = Recall\n",
    "specificity = (tn / (tn + fp)).mean()  # Avoid division by zero\n",
    "non_error_rate = (tp + tn) / cm.sum()  # Non Error Rate\n",
    "\n",
    "print(f\"AUPR Score: {aupr:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Non-Error Rate: {non_error_rate.mean():.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
